# EXP-3-PROMPT-ENGINEERING-

## Aim: 
Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: 
ChatGPT, Claude, Bard, Cohere Command, and Meta
Experiment:
Within a specific use case (e.g., summarizing text, answering technical questions), compare the performance, user experience, and response quality of prompting tools across these different AI platforms.

## Algorithm:
Step 1 : Analyze the prompt to define the report's topic, scope, and requirements.

Step 2 : Create a detailed, multi-section outline to structure the report's content.

Step 3 : Synthesize key information for each platform and use case.

Step 4 : Generate each report section sequentially, from the introduction to the conclusion.

Step 5 : Formulate prompts to create relevant images for each section.

Step 6 : Interleave the generated images within the report's text.

Step 7 : Review and refine the complete report to ensure it meets all instructions

## Prompt:
Create a detailed 20-page analytical study titled “2024 Review of Prompting Systems in Major AI Models: ChatGPT, Claude, Bard, Cohere Command, and Meta Models.”

Evaluate them in the contexts of large-document summarization and technical Q&A accuracy.

Include performance, UX, and response-quality comparisons for each model.

Discuss prompting styles, strengths/limits, and provide templates (system prompts, few-shot, chained, RAG-based).

Add qualitative ratings on correctness, coherence, instruction-following, long-context stability, developer experience, and safety.

Structure the report with academic sections like Executive Summary, Methods, Platform Analysis, Experiments, Recommendations, Appendices.

Conclude with best practices and insights on the evolution of prompt engineering in 2024.

## Output

[PROMPT - 3.pdf](https://github.com/user-attachments/files/23559634/PROMPT.-.3.pdf)


## Result


The experiment produced a 20-page comparative analysis of ChatGPT, Claude, Bard, Cohere Command, and Meta models, evaluating their performance in long-document summarization and technical question answering.

Each platform was tested, scored, and compared on correctness, coherence, instruction-following, long-context handling, response quality, safety, and developer experience, with example prompt templates included.

The report concluded with experimental insights and best-practice guidelines, showing how prompting techniques across AI platforms matured in 2024.

